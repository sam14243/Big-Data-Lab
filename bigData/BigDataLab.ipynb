{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "317c309e-2879-4377-b4af-86efaf10f13f",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "List of basic linux commands\n",
    "\n",
    "## Exercise 2\n",
    "Install and configure hadoop\n",
    "\n",
    "## Excercise 3\n",
    "Implement a simple map-reduce code for the wordcount problem using Java/Python (Create the jar files and run the code using HDFS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584055e2-b2a6-4cfb-9b5f-a1685a71ab31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed653305-f943-471a-9bd5-0b9ff5696d4c",
   "metadata": {},
   "source": [
    "## Excercise 4\n",
    "Implement map reduce for NCDC weather dataset using hadoop find the max and min temperature\n",
    "\n",
    "Implement apriori algorithm using map reduce paradigm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dedefb7-74c6-44c4-9e49-28cee2b1e9bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a5d73b-4e7e-4690-b3d0-29baa89cf9dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "52f8a201-1099-465a-b965-ed730e88e194",
   "metadata": {},
   "source": [
    "## Excercise 5\n",
    "Run the wordcount program using pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "26697682-64d8-4874-884a-b390ae14c147",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/03 19:00:30 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apache: 1\n",
      "Spark: 4\n",
      "is: 4\n",
      "a: 3\n",
      "fast: 1\n",
      "and: 5\n",
      "general-purpose: 1\n",
      "cluster: 1\n",
      "computing: 1\n",
      "system.: 1\n",
      "provides: 1\n",
      "an: 1\n",
      "interface: 1\n",
      "for: 3\n",
      "programming: 1\n",
      "entire: 1\n",
      "clusters: 1\n",
      "with: 1\n",
      "implicit: 1\n",
      "data: 3\n",
      "parallelism: 1\n",
      "fault: 1\n",
      "tolerance.: 1\n",
      "It: 1\n",
      "designed: 1\n",
      "to: 2\n",
      "make: 1\n",
      "processing: 1\n",
      "faster: 1\n",
      "more: 1\n",
      "accessible.: 1\n",
      "The: 1\n",
      "ecosystem: 1\n",
      "includes: 1\n",
      "components: 1\n",
      "SQL,: 1\n",
      "streaming: 1\n",
      "data,: 1\n",
      "machine: 1\n",
      "learning,: 1\n",
      "graph: 1\n",
      "processing.: 1\n",
      "With: 1\n",
      "Spark,: 1\n",
      "you: 1\n",
      "can: 1\n",
      "process: 1\n",
      "large: 1\n",
      "datasets: 1\n",
      "efficiently,: 1\n",
      "scaling: 1\n",
      "from: 1\n",
      "single: 1\n",
      "server: 1\n",
      "thousands: 1\n",
      "of: 1\n",
      "machines.: 1\n",
      "used: 1\n",
      "widely: 1\n",
      "across: 1\n",
      "industries: 1\n",
      "top: 1\n",
      "choice: 1\n",
      "big: 1\n",
      "analytics.: 1\n",
      ": 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Initialize Spark Context\n",
    "sc = SparkContext(\"local\", \"WordCount\")\n",
    "\n",
    "# Read input file (change 'input.txt' to your file path)\n",
    "input_file = sc.textFile(\"new.txt\")\n",
    "\n",
    "# Split each line into words, flatten the list, and map to (word, 1) pairs\n",
    "word_counts = input_file.flatMap(lambda line: line.split(\" \")) \\\n",
    "                        .map(lambda word: (word, 1)) \\\n",
    "                        .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Collect results and print them\n",
    "results = word_counts.collect()\n",
    "\n",
    "for word, count in results:\n",
    "    print(f\"{word}: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b649c100-8eac-446b-8256-944c091fda0e",
   "metadata": {},
   "source": [
    "## Excercise 5\n",
    "Run wordcount program using pyspark. Use the movielens dataset and find the ratings distribution for each movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1c619fc-4114-420f-9701-76c4f5e8def5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/03 14:17:13 WARN Utils: Your hostname, malathi-virtual-machine resolves to a loopback address: 127.0.1.1; using 192.168.20.128 instead (on interface ens33)\n",
      "24/11/03 14:17:13 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/03 14:17:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/11/03 14:17:17 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 27145\n",
      "1 6110\n",
      "2 11370\n",
      "4 34174\n",
      "5 21201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "import collections\n",
    "\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"movielens\")\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "lines = sc.textFile(\"u.data\")\n",
    "ratings = lines.map(lambda x: x.split()[2])\n",
    "result = ratings.countByValue()\n",
    "\n",
    "for key, value in result.items():\n",
    "    print(key,value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "804d90fa-7fb3-43e6-b488-cdbb891f82dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9fb689-9899-4321-bba0-7e34cb16f7ef",
   "metadata": {},
   "source": [
    "## Exercise 6\n",
    "Use the \"friends_test\" dataset. Col1 is ID. Col2 is name, Col3 is age, Col4 is num of friends. Understand mapvalues function of RDD in spark and find the average number of friends for each unique age present in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5b4505b-6b4a-4252-8be1-916223c3a64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "441db3a2-8a2c-4b91-9bc3-2bcc4e527c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/03 17:39:00 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"friends test\") \\\n",
    "    .config(\"spark.memory.offHeap.enabled\", \"true\") \\\n",
    "    .config(\"spark.memory.offHeap.size\", \"2g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7a8dc2a-5768-4376-b631-3da72c8d0a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('friends_test.csv', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "590001f2-68fd-43ae-8428-60a26521455e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "FileScan csv [_c0#17,_c1#18,_c2#19,_c3#20] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/hadoop/friends_test.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<_c0:string,_c1:string,_c2:string,_c3:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dae2807e-d8bb-4bcd-afc9-7bc000d46f8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0='0', _c1='Will', _c2='33', _c3='385'),\n",
       " Row(_c0='1', _c1='Jean-Luc', _c2='26', _c3='2'),\n",
       " Row(_c0='2', _c1='Hugh', _c2='55', _c3='221'),\n",
       " Row(_c0='3', _c1='Deanna', _c2='40', _c3='465'),\n",
       " Row(_c0='4', _c1='Quark', _c2='68', _c3='21')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81c94384-81f2-41bd-9181-2add94ce7a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+---+\n",
      "|_c0|     _c1|_c2|_c3|\n",
      "+---+--------+---+---+\n",
      "|  0|    Will| 33|385|\n",
      "|  1|Jean-Luc| 26|  2|\n",
      "|  2|    Hugh| 55|221|\n",
      "|  3|  Deanna| 40|465|\n",
      "|  4|   Quark| 68| 21|\n",
      "|  5|  Weyoun| 59|318|\n",
      "|  6|  Gowron| 37|220|\n",
      "|  7|    Will| 54|307|\n",
      "|  8|  Jadzia| 38|380|\n",
      "|  9|    Hugh| 27|181|\n",
      "| 10|     Odo| 53|191|\n",
      "| 11|     Ben| 57|372|\n",
      "| 12|   Keiko| 54|253|\n",
      "| 13|Jean-Luc| 56|444|\n",
      "| 14|    Hugh| 43| 49|\n",
      "| 15|     Rom| 36| 49|\n",
      "| 16|  Weyoun| 22|323|\n",
      "| 17|     Odo| 35| 13|\n",
      "| 18|Jean-Luc| 45|455|\n",
      "| 19|  Geordi| 60|246|\n",
      "+---+--------+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71ac16d6-638c-471c-a9fe-02d7d43b4f39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e953d0f0-71e3-43f1-99cd-9a43df077d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c008671-fc48-4116-95e3-ea203e0b3563",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/03 17:45:59 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# RDD approach for average number of friends\n",
    "conf = SparkConf().setAppName(\"Basicapp\").setMaster(\"local[*]\")\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eba11934-07e7-46ee-a0d0-0ade5c65adef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0,Will,33,385'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.textFile(\"friends_test.csv\")\n",
    "rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5534ba69-000d-4f58-abfe-8fcf2536db0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', 'Will', '33', '385']\n",
      "['1', 'Jean-Luc', '26', '2']\n",
      "['2', 'Hugh', '55', '221']\n",
      "['3', 'Deanna', '40', '465']\n",
      "['4', 'Quark', '68', '21']\n"
     ]
    }
   ],
   "source": [
    "rdd_split = rdd.map(lambda line: line.split(\",\"))\n",
    "for row in rdd_split.take(5):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "756ef55e-c3bc-4759-b9d4-eb438fb32f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_friends_rdd = rdd_split.map(lambda row: (int(row[2]), (int(row[3]), 1)))\n",
    "\n",
    "sum_count_rdd = age_friends_rdd.reduceByKey(lambda a, b: (a[0]+b[0], a[1]+b[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e491378-b101-4eb5-acaf-6decfe9a1120",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "avg_friends_by_age = sum_count_rdd.mapValues(lambda x: x[0]/x[1])\n",
    "sorted_avg_friends_by_age = avg_friends_by_age.sortByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "48b8a362-4849-4a3d-a95b-6df68ab9d89e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age: 18, Average Number of Friends:  343.38\n",
      "Age: 19, Average Number of Friends:  213.27\n",
      "Age: 20, Average Number of Friends:  165.00\n",
      "Age: 21, Average Number of Friends:  350.88\n",
      "Age: 22, Average Number of Friends:  206.43\n",
      "Age: 23, Average Number of Friends:  246.30\n",
      "Age: 24, Average Number of Friends:  233.80\n",
      "Age: 25, Average Number of Friends:  197.45\n",
      "Age: 26, Average Number of Friends:  242.06\n",
      "Age: 27, Average Number of Friends:  228.12\n",
      "Age: 28, Average Number of Friends:  209.10\n",
      "Age: 29, Average Number of Friends:  215.92\n",
      "Age: 30, Average Number of Friends:  235.82\n",
      "Age: 31, Average Number of Friends:  267.25\n",
      "Age: 32, Average Number of Friends:  207.91\n",
      "Age: 33, Average Number of Friends:  325.33\n",
      "Age: 34, Average Number of Friends:  245.50\n",
      "Age: 35, Average Number of Friends:  211.62\n",
      "Age: 36, Average Number of Friends:  246.60\n",
      "Age: 37, Average Number of Friends:  249.33\n",
      "Age: 38, Average Number of Friends:  193.53\n",
      "Age: 39, Average Number of Friends:  169.29\n",
      "Age: 40, Average Number of Friends:  250.82\n",
      "Age: 41, Average Number of Friends:  268.56\n",
      "Age: 42, Average Number of Friends:  303.50\n",
      "Age: 43, Average Number of Friends:  230.57\n",
      "Age: 44, Average Number of Friends:  282.17\n",
      "Age: 45, Average Number of Friends:  309.54\n",
      "Age: 46, Average Number of Friends:  223.69\n",
      "Age: 47, Average Number of Friends:  233.22\n",
      "Age: 48, Average Number of Friends:  281.40\n",
      "Age: 49, Average Number of Friends:  184.67\n",
      "Age: 50, Average Number of Friends:  254.60\n",
      "Age: 51, Average Number of Friends:  302.14\n",
      "Age: 52, Average Number of Friends:  340.64\n",
      "Age: 53, Average Number of Friends:  222.86\n",
      "Age: 54, Average Number of Friends:  278.08\n",
      "Age: 55, Average Number of Friends:  295.54\n",
      "Age: 56, Average Number of Friends:  306.67\n",
      "Age: 57, Average Number of Friends:  258.83\n",
      "Age: 58, Average Number of Friends:  116.55\n",
      "Age: 59, Average Number of Friends:  220.00\n",
      "Age: 60, Average Number of Friends:  202.71\n",
      "Age: 61, Average Number of Friends:  256.22\n",
      "Age: 62, Average Number of Friends:  220.77\n",
      "Age: 63, Average Number of Friends:  384.00\n",
      "Age: 64, Average Number of Friends:  281.33\n",
      "Age: 65, Average Number of Friends:  298.20\n",
      "Age: 66, Average Number of Friends:  276.44\n",
      "Age: 67, Average Number of Friends:  214.62\n",
      "Age: 68, Average Number of Friends:  269.60\n",
      "Age: 69, Average Number of Friends:  235.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "for age, avg_friends in sorted_avg_friends_by_age.collect():\n",
    "    print(f\"Age: {age}, Average Number of Friends: {avg_friends: .2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bbaec8-cc96-4d8c-89ca-4a5b9bb6522f",
   "metadata": {},
   "source": [
    "## Exercise 6\n",
    "Use the \"temp.csv\" dataset. Column headers are present in the dataset. Understand filter operations and filter out only the \"TMIN\" values from the \"desc\" column. With the resultant data (RDD) find the following\n",
    "a. minimum temperature (overall)\n",
    "b. minimum temperature for every ItemID\n",
    "c. minimum temperature for every StationID\n",
    "\n",
    "Use the same dataset, filter onlt \"TMAX\" column and find the maximum temperatures just like the ones mentioned above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a262afe0-d174-4194-93da-46b1ef36d6b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/03 18:18:31 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "conf = SparkConf().setAppName(\"temp_dataset\").setMaster(\"local[*]\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a8ffa86d-2c7e-49e8-aabc-71f2d4bbb5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'itemID,stationID,desc,temp'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.textFile(\"temp.csv\")\n",
    "rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7fdce389-c599-4e1d-ada5-8a94fbf703bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rdd_header = rdd.first()\n",
    "rdd_filter = rdd.filter(lambda row: row!= rdd_header)\n",
    "rdd_data = rdd_filter.map(lambda row: row.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a64840e4-1dbf-421b-90d2-980b8dbaa247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rdd_display(x, threshold=5):\n",
    "    count=0\n",
    "    for i in x.collect():\n",
    "        print(i)\n",
    "        count+=1\n",
    "        if(count>threshold):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f24a73de-2b6d-4cb8-b6e1-4a1d5605bc21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ITE00100554', '18000101', 'TMAX', '-75']\n",
      "['ITE00100554', '18000101', 'TMIN', '-148']\n",
      "['GM000010962', '18000101', 'PRCP', '0']\n",
      "['EZE00100082', '18000101', 'TMAX', '-86']\n",
      "['EZE00100082', '18000101', 'TMIN', '-135']\n",
      "['ITE00100554', '18000102', 'TMAX', '-60']\n"
     ]
    }
   ],
   "source": [
    "rdd_display(rdd_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f8b5f864-258a-4c09-9eee-ddecab1801c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ITE00100554', '18000101', 'TMIN', '-148']\n",
      "['EZE00100082', '18000101', 'TMIN', '-135']\n",
      "['ITE00100554', '18000102', 'TMIN', '-125']\n",
      "['EZE00100082', '18000102', 'TMIN', '-130']\n",
      "['ITE00100554', '18000103', 'TMIN', '-46']\n",
      "['EZE00100082', '18000103', 'TMIN', '-73']\n"
     ]
    }
   ],
   "source": [
    "rdd_TMIN_filter = rdd_data.filter(lambda row: row[2]==\"TMIN\")\n",
    "rdd_display(rdd_TMIN_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dacef1db-c513-42b7-b67e-dbc08260a66f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum temperature overall: -148\n"
     ]
    }
   ],
   "source": [
    "rdd_min_overall = rdd_TMIN_filter.map(lambda x: int(x[3])).reduce(lambda a, b: a if a < b else b)\n",
    "print(\"Minimum temperature overall:\", rdd_min_overall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f3db6ca9-21c6-4adb-a4ea-c78ec7a5cc58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum temperature by itemID\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ITE00100554', -148)\n",
      "('EZE00100082', -135)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rdd_min_itemID = rdd_TMIN_filter.map(lambda x:(x[0],int(x[3]))).reduceByKey(lambda a, b: a if a < b else b)\n",
    "print(\"minimum temperature by itemID\")\n",
    "rdd_display(rdd_min_itemID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cf488a6a-acce-4f73-9311-274371c03e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum temperature by StationID\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('18000102', -130)\n",
      "('18000104', -74)\n",
      "('18000106', -57)\n",
      "('18000110', -75)\n",
      "('18000111', -62)\n",
      "('18000112', -60)\n",
      "('18000114', -35)\n",
      "('18000115', -23)\n",
      "('18000116', -37)\n",
      "('18000117', -35)\n",
      "('18000118', 9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rdd_min_stationID = rdd_TMIN_filter.map(lambda x:(x[1],int(x[3]))).reduceByKey(lambda a, b: a if a < b else b)\n",
    "print(\"minimum temperature by StationID\")\n",
    "rdd_display(rdd_min_stationID,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6a9d26ac-a2bc-49db-90b4-03ed4ba7e345",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ITE00100554', '18000101', 'TMAX', '-75']\n",
      "['EZE00100082', '18000101', 'TMAX', '-86']\n",
      "['ITE00100554', '18000102', 'TMAX', '-60']\n",
      "['EZE00100082', '18000102', 'TMAX', '-44']\n",
      "['ITE00100554', '18000103', 'TMAX', '-23']\n",
      "['EZE00100082', '18000103', 'TMAX', '-10']\n"
     ]
    }
   ],
   "source": [
    "rdd_TMAX_filter = rdd_data.filter(lambda row:row[2]==\"TMAX\")\n",
    "rdd_display(rdd_TMAX_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ea309184-5846-4e57-a694-5d106f091e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum temperature overall: 323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rdd_max_overall = rdd_TMAX_filter.map(lambda x: int(x[3])).reduce(lambda a, b: a if a > b else b)\n",
    "print(\"Maximum temperature overall:\", rdd_max_overall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "081c6199-65c7-488f-8b82-6c56eebcbdd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum temperature by itemID\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ITE00100554', 323)\n",
      "('EZE00100082', 323)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rdd_max_itemID = rdd_TMAX_filter.map(lambda x:(x[0],int(x[3]))).reduceByKey(lambda a, b: a if a > b else b)\n",
    "print(\"maximum temperature by itemID\")\n",
    "rdd_display(rdd_max_itemID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6d02abe7-9f6d-45a4-a102-ab978b73eac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum temperature by StationID\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('18000102', -44)\n",
      "('18000104', 0)\n",
      "('18000106', 13)\n",
      "('18000110', 46)\n",
      "('18000111', 66)\n",
      "('18000112', 41)\n",
      "('18000114', 41)\n",
      "('18000115', 54)\n",
      "('18000116', 56)\n",
      "('18000117', 84)\n",
      "('18000118', 59)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rdd_max_stationID = rdd_TMAX_filter.map(lambda x:(x[1],int(x[3]))).reduceByKey(lambda a, b: a if a > b else b)\n",
    "print(\"maximum temperature by StationID\")\n",
    "rdd_display(rdd_max_stationID,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "57426096-34da-48b9-807e-c0bd30ce7ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b332cd8c-a229-408a-b591-e33faa33f431",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
